{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhbW1jHZLlPC"
   },
   "source": [
    "# Lab 15: Data assimilation with remote sensing\n",
    "\n",
    "**Purpose:** The purpose of this lab is to familiarize students with using remote sensing/geospatial data for setting up a hydrologic model as well as assimilating remote sensing data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUpP2LLdwQEq"
   },
   "outputs": [],
   "source": [
    "# connect Google Drive so we can use exported data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVg-ZH8NMl75"
   },
   "outputs": [],
   "source": [
    "# install geemap package for visualizing ee results\n",
    "!pip install geemap filterpy HydroErr &> install.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3ic7dQiwLnF"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/KMarkert/sacsma.git -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1TdjRcjM-WI"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQjeaoZYMrh9"
   },
   "outputs": [],
   "source": [
    "# import ee api and geemap package\n",
    "import ee\n",
    "import math\n",
    "import geemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from geemap import colormaps as cmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qitu8Ml0Mrer"
   },
   "outputs": [],
   "source": [
    "# try to initalize an ee session\n",
    "# if not authenticated then run auth workflow and initialize\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5Ydok4QS0oM"
   },
   "source": [
    "## Forcing data for model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbyc3xqyDbUe"
   },
   "source": [
    "USGS Streamflow data: https://waterdata.usgs.gov/nwis/inventory?agency_code=USGS&site_no=10153100\n",
    "\n",
    "NCRS SNOWTEL data: https://wcc.sc.egov.usda.gov/nwcc/site?sitenum=1223 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Rqtfi5J2-Ad"
   },
   "outputs": [],
   "source": [
    "# change to your prefered study period\n",
    "# must be within the time range of available observed data\n",
    "START_TIME = '2009-01-01'\n",
    "END_TIME = '2021-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vaAIqLNWSTEL"
   },
   "outputs": [],
   "source": [
    "# specify where the gauge is located so we can filter the basin by location\n",
    "gauge_lat,gauge_lon = 40.179, -111.639\n",
    "gauge_pt = ee.Geometry.Point([gauge_lon,gauge_lat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJe__A3mMrbt"
   },
   "outputs": [],
   "source": [
    "# load in the watershed feature collection\n",
    "watersheds = ee.FeatureCollection(\"USGS/WBD/2017/HUC10\")\n",
    "\n",
    "# filter by gauge location\n",
    "hobble_creek = ee.Feature(watersheds.filterBounds(gauge_pt).first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOGeiOAX2VsJ"
   },
   "outputs": [],
   "source": [
    "Map = geemap.Map()\n",
    "\n",
    "Map.centerObject(hobble_creek)\n",
    "\n",
    "Map.addLayer(hobble_creek,{},\"Hobble Creek Basin\")\n",
    "Map.addLayer(gauge_pt,{\"color\":\"yellow\",},\"Gauge\")\n",
    "\n",
    "Map.addLayerControl()\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvakPirM0j-1"
   },
   "outputs": [],
   "source": [
    "# specify band names we want\n",
    "metBands = ['prcp','tmin','tmax',]\n",
    "\n",
    "daymet_col = ee.ImageCollection(\"NASA/ORNL/DAYMET_V4\")\n",
    "\n",
    "# filter the collection by date and select the bands on interest\n",
    "met_col = (\n",
    "    daymet_col\n",
    "    .filterDate(START_TIME, ee.Date(END_TIME).advance(1,'day'))\\\n",
    "    .select(metBands)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUONJEQU24Cf"
   },
   "outputs": [],
   "source": [
    "# define a function to calculated time series for the basin\n",
    "def get_timeseries(img):\n",
    "    results = img.reduceRegion(\n",
    "        reducer = ee.Reducer.mean(),\n",
    "        geometry = hobble_creek.geometry(1e4),\n",
    "        scale = img.select([0]).projection().nominalScale()\n",
    "    )\n",
    "\n",
    "    return img.set(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUaGjCwR3nRy"
   },
   "outputs": [],
   "source": [
    "# get a time series of meteorological data\n",
    "met_col_timeseries = met_col.map(get_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3iu8318_xHs"
   },
   "outputs": [],
   "source": [
    "# get the average elevation for the basin\n",
    "elv = get_timeseries(ee.Image(\"NASA/NASADEM_HGT/001\"))\n",
    "\n",
    "elv_avg = elv.get(\"elevation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhbJrRHqACsp"
   },
   "outputs": [],
   "source": [
    "elv_avg.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mr330Qq94bs5"
   },
   "outputs": [],
   "source": [
    "# define a function to convert the images to features\n",
    "def img_to_feature(img):\n",
    "    img = img.set(\"date\",img.date().format(\"YYYY-MM-dd\"))\n",
    "    geo = hobble_creek.centroid(1e4).set(\"elev\",elv_avg)\n",
    "    return geo.copyProperties(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAP2HYIK4bv-"
   },
   "outputs": [],
   "source": [
    "# convert images to features\n",
    "timeseries_table = met_col_timeseries.map(img_to_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2ur-wSU4b6L"
   },
   "outputs": [],
   "source": [
    "timeseries_table.first().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBEhow2L647B"
   },
   "outputs": [],
   "source": [
    "# run task for met data\n",
    "task = ee.batch.Export.table.toDrive(\n",
    "    collection = timeseries_table,\n",
    "    description = \"meterological_timeseries_export\",\n",
    "    fileNamePrefix= \"hobble_creek_met\",\n",
    "    folder = \"hobble_creek_model\",\n",
    "    fileFormat = \"CSV\"\n",
    ")\n",
    "\n",
    "task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXHda04Gc2tN"
   },
   "source": [
    "## State data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NV8CPZUG9BO1"
   },
   "outputs": [],
   "source": [
    "soilmoisture = ee.ImageCollection(\"NASA_USDA/HSL/SMAP10KM_soil_moisture\").filterDate(\"2016-01-01\",END_TIME)\n",
    "swe = daymet_col.select(\"swe\").filterDate(\"2016-01-01\",END_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMjGRshr8sts"
   },
   "outputs": [],
   "source": [
    "# Define an allowable time difference: ten days in milliseconds.\n",
    "half_day_millis = 24 * 60 * 60 * 1000\n",
    "\n",
    "# Create a time filter to define a match as overlapping timestamps.\n",
    "time_filter = ee.Filter.Or(\n",
    "    # use max difference filter to specify only one day difference\n",
    "    # checks one day on either side of observation\n",
    "    ee.Filter.maxDifference(\n",
    "        difference= half_day_millis,\n",
    "        leftField= 'system:time_start',\n",
    "        rightField= 'system:time_start'\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umWLhSET9PoW"
   },
   "outputs": [],
   "source": [
    "# Define the join.\n",
    "# this is \"saveBest\" which will give us the image closest in time to what we want\n",
    "state_join = ee.Join.saveBest(\n",
    "  matchKey= 'swe', # this will be the name of the result in the collection\n",
    "  measureKey= 'timeDiff'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akbM9bBQ9UJz"
   },
   "outputs": [],
   "source": [
    "# Apply the join.\n",
    "# uses soil_moisture as the collection to join to and applies filter on surface reflectance data\n",
    "joined_states = ee.ImageCollection(state_join.apply(soilmoisture, swe, time_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmJNQ7Xl9d2y"
   },
   "outputs": [],
   "source": [
    "# define a function to unpack the joined properties\n",
    "def unpack_join(img):\n",
    "    return img.addBands(img.get(\"swe\"))\n",
    "\n",
    "state_imgs = joined_states.map(unpack_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Um8lPqof9qTS"
   },
   "outputs": [],
   "source": [
    "# get the time series \n",
    "state_timeseries = state_imgs.map(get_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMedQvim9wd1"
   },
   "outputs": [],
   "source": [
    "# convert images to faeture collection\n",
    "state_table = state_timeseries.map(img_to_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwJRl0Mm_eOe"
   },
   "outputs": [],
   "source": [
    "state_table.first().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yicJLigA92sY"
   },
   "outputs": [],
   "source": [
    "# run task to export state information\n",
    "task = ee.batch.Export.table.toDrive(\n",
    "    collection = state_table,\n",
    "    description = \"state_timeseries_export\",\n",
    "    fileNamePrefix= \"hobble_creek_state\",\n",
    "    folder = \"hobble_creek_model\",\n",
    "    fileFormat = \"CSV\"\n",
    ")\n",
    "\n",
    "task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygGqNgoi97cA"
   },
   "outputs": [],
   "source": [
    "ee.batch.Task.list()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arvQHb7m_HKs"
   },
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NeO2RxW96tV"
   },
   "outputs": [],
   "source": [
    "from sacsma.simulations import Simulation\n",
    "import filterpy.kalman as kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4g193De9PB1"
   },
   "outputs": [],
   "source": [
    "# read in the meteorological forcing data\n",
    "forcings_df = pd.read_csv(\"/content/drive/MyDrive/hobble_creek_model/hobble_creek_met.csv\",index_col=\"date\")\n",
    "forcings_df.index = forcings_df.index.astype('datetime64[ns]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9YAyR4WuiNQ"
   },
   "outputs": [],
   "source": [
    "# read in the state data from remote sensing data\n",
    "state_df = pd.read_csv(\"/content/drive/MyDrive/hobble_creek_model/hobble_creek_state.csv\",index_col=\"date\")\n",
    "state_df.index = state_df.index.astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNDaLkEny1PU"
   },
   "outputs": [],
   "source": [
    "# read in the observed streamflow data\n",
    "obs = pd.read_csv(\"/content/drive/MyDrive/hobble_creek_model/USGS_10153100_streamflow.csv\",index_col=\"datetime\")\n",
    "obs.index = obs.index.astype('datetime64[ns]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtOX21a-4I2O"
   },
   "outputs": [],
   "source": [
    "# plot the forcing data\n",
    "axs = forcings_df[[\"prcp\",\"tmin\",\"tmax\"]].plot(figsize=(10,7),subplots=True);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEd6nD384Nz_"
   },
   "outputs": [],
   "source": [
    "# extract out the two time periods from forcing dataset\n",
    "forcings_cal = forcings_df.loc[forcings_df.index < \"2016-01-01\"]\n",
    "forcings_assim = forcings_df.loc[forcings_df.index >= \"2016-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PO7iBxqPwuC"
   },
   "outputs": [],
   "source": [
    "# extract out the two time periods from observed dataset\n",
    "obs_cal = obs.loc[obs.index < \"2016-01-01\"]\n",
    "obs_assim = obs.loc[obs.index >= \"2016-01-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJm_BZfpWRcS"
   },
   "source": [
    "Next we need to define the model parameters we are going to use. Each part of the model has their own set of parameters: 1) the snow model, 2) the land surface model, and 3) the routing model.\n",
    "\n",
    "An important concept of modeling is calibration to get the parameter right. As simplified calibration was completed to get an initial guess at parameters for our basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ip2bmaSKGXf"
   },
   "outputs": [],
   "source": [
    "# snow model parameters\n",
    "snow_pars = np.array([\n",
    "    1.46863,    # snow correction factor\n",
    "    1.37133,    # max of the seasonally varying non-rain melt factor\n",
    "    0.508376,   # min of the seasonally varying non-rain melt factor\n",
    "    0.0656106,  # average wind function during rain-on-snow events\n",
    "    1.24992,    # temperature threshold for snow vs rain\n",
    "    0.206033,   # negative melt factor\n",
    "    0.165462,   # used to compute an antecedent temperature index\n",
    "    -0.438637,  # the base temperature used to determine the temperature gradient for non-rain melt computations\n",
    "    0.531435,   # controls the maximum amount of liquid water that can be retained within the snow cover (decimal fraction)\n",
    "    0.411856,   # controls the amount of melt per day that occurs at the snow-soil interface [1/day]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rINLgRJKuxTH"
   },
   "outputs": [],
   "source": [
    "# land surface model parameters\n",
    "ls_pars = np.array([\n",
    "    51.1675,    # Upper zone tension water capacity [mm]\n",
    "    63.9568,    # Upper zone free water capacity [mm]\n",
    "    234.478,    # Lower zone tension water capacity [mm]\n",
    "    200.876,    # Lower zone primary free water capacity [mm]\n",
    "    61.6775,    # Lower zone supplementary free water capacity [mm]\n",
    "    0.107242,   # Additional impervious areas (decimal fraction)\n",
    "    0.351116,   # Upper zone free water lateral depletion rate [1/day]\n",
    "    0.00537984, # Lower zone primary free water depletion rate [1/day]\n",
    "    0.0623334,  # Lower zone supplementary free water depletion rate [1/day]\n",
    "    138.602,    # Percolation demand scale parameter [-]\n",
    "    2.95144,    # Percolation demand shape parameter [-]\n",
    "    0.0374366,  # Impervious fraction of the watershed area (decimal fraction)\n",
    "    0.300625,   # Percolating water split parameter (decimal fraction)\n",
    "    0.0841843,  # Riparian vegetation area (decimal fraction)\n",
    "    0.274862,   # The ratio of deep recharge to channel base flow [-]\n",
    "    0.447683,   # Fraction of lower zone free water not transferrable (decimal fraction)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDqN6fCZuzHi"
   },
   "outputs": [],
   "source": [
    "# routing model parameters\n",
    "routing_pars = np.array([\n",
    "    13.1234,    # Unit Hydrograph shape parameter\n",
    "    19.7153,    # Unit Hydrograph scale parameter\n",
    "    3.11245,    # wave velocity in the linearized Saint-Venant equation(m/s)\n",
    "    1004.67,    # diffusivity in the linearized Saint-Venant equation(m2/s)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvoZ_61miB6J"
   },
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLSfiILBih1j"
   },
   "outputs": [],
   "source": [
    "# instantiate a model run using the forcings from calibration period\n",
    "model_init = Simulation(forcings_cal,forcings_df[\"elev\"][0],snow_pars,ls_pars,routing_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_9YIAPAiIw0"
   },
   "outputs": [],
   "source": [
    "# execute the model to get discharge\n",
    "# this runs the snow, land surface, and routing model\n",
    "q = model_init.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G86NPc32iao3"
   },
   "outputs": [],
   "source": [
    "# get the date information as arrays\n",
    "cal_dates = forcings_cal.index.values.astype(np.datetime64)\n",
    "assim_dates = forcings_assim.index.values.astype(np.datetime64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTN895eVYxvJ"
   },
   "outputs": [],
   "source": [
    "# create a dataframe of the simulated values\n",
    "sim_df = pd.DataFrame({\"simulated\":q},index=cal_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbAPzpomYvZb"
   },
   "outputs": [],
   "source": [
    "# join the dataframes together to align dates\n",
    "joined = pd.concat([sim_df,obs_cal[\"discharge\"]], axis=1)\n",
    "joined = joined.loc[(joined.index >= \"2010-01-01\") & (joined.index < \"2016-01-01\")].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGJ1C98-iEDf"
   },
   "outputs": [],
   "source": [
    "# plot the observed vs simulated\n",
    "ax = joined.plot(figsize=(10,7))\n",
    "ax.set_ylabel(\"Discharge [cms]\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKtQduZ4XvKc"
   },
   "outputs": [],
   "source": [
    "from HydroErr import nse\n",
    "\n",
    "nse_cal = nse(joined[\"simulated\"],joined[\"discharge\"])\n",
    "\n",
    "print(f\"NSE: {nse_cal:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwK8vwQYh-lH"
   },
   "source": [
    "## One Dimensional Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHzkutXUvqms"
   },
   "outputs": [],
   "source": [
    "# extract out soil moisture and swe values from previous run\n",
    "sm = model_init.sm\n",
    "we = model_init.we\n",
    "swe = np.sum(we,axis=0)\n",
    "\n",
    "# extract out soil moisture and swe values from observed\n",
    "sm_obs = state_df[\"ssm\"].values\n",
    "swe_obs = state_df[\"swe\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxIAoOMavk2c"
   },
   "outputs": [],
   "source": [
    "# calculate variances of the simulated and observed variables\n",
    "# note: simulated variances are from previous model run\n",
    "#       and the observed variances are from observed period\n",
    "sm_P = np.var(sm)\n",
    "swe_P = np.var(swe)\n",
    "\n",
    "sm_R = np.var(sm_obs)\n",
    "swe_R = np.var(swe_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apE7Ighwvt8f"
   },
   "outputs": [],
   "source": [
    "# instantiate another model that we will use to assimilate the data into\n",
    "model_assim = Simulation(\n",
    "    forcings_assim,\n",
    "    forcings_df[\"elev\"][0],\n",
    "    snow_pars,\n",
    "    ls_pars,\n",
    "    routing_pars, \n",
    "    ls_state=model_init.ls_state, \n",
    "    snow_state=model_init.snow_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7T1DhEMvwlJ"
   },
   "outputs": [],
   "source": [
    "# get the number of time steps\n",
    "n_steps = range(model_assim.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4RHpsfviDxQ"
   },
   "outputs": [],
   "source": [
    "# manually step through each model iteration\n",
    "# if there are data from the observations, then we will \n",
    "# update the state using the Kalman Filter\n",
    "for i in n_steps:\n",
    "    # run the model time step\n",
    "    model_assim.step(i)\n",
    "\n",
    "    if model_assim.dates[i] in state_df.index:\n",
    "        idx = np.squeeze(np.where(state_df.index.astype('datetime64[ns]') == model_assim.dates[i]))\n",
    "\n",
    "        model_assim.ls_state[0], sm_p = kf.update(x=model_assim.ls_state[0], P=sm_P, z=sm_obs[idx], R=sm_R)\n",
    " \n",
    "        we = np.array([model_assim.snow_state[0],model_assim.snow_state[2]])\n",
    "        x_swe,swe_p = kf.update(x=np.sum(we), P=swe_P, z=swe_obs[idx], R=swe_R)\n",
    "\n",
    "        if np.sum(we)>0:\n",
    "            swe_weights = (we/np.sum(we))\n",
    "        else:\n",
    "            swe_weights = np.array([0.5,0.5])\n",
    "\n",
    "        model_assim.snow_state[0],model_assim.snow_state[2] = x_swe * swe_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZC9FEztBx1XK"
   },
   "outputs": [],
   "source": [
    "from sacsma import routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRgMS--dx6zi"
   },
   "outputs": [],
   "source": [
    "# extract out the runoff components from the LS model\n",
    "assim_runoff = model_assim.runoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aza-uITjxH6B"
   },
   "outputs": [],
   "source": [
    "# run the routing model\n",
    "flowlength = 71634.0\n",
    "assim_direct,assim_base = routing.lohmann(assim_runoff[1,:],assim_runoff[2,:], flowlength, routing_pars)\n",
    "assim_q = assim_direct + assim_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QEsS8AVwfwm"
   },
   "outputs": [],
   "source": [
    "# setup an open loop simulation to compare results\n",
    "model_openloop = Simulation(\n",
    "    forcings_assim,\n",
    "    forcings_df[\"elev\"][0],\n",
    "    snow_pars,\n",
    "    ls_pars,\n",
    "    routing_pars, \n",
    "    ls_state=model_init.ls_state, \n",
    "    snow_state=model_init.snow_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xkm8M3fvwqpV"
   },
   "outputs": [],
   "source": [
    "# run the open loop simulation\n",
    "openloop_q = model_openloop.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zh_EqhJwv7Yq"
   },
   "outputs": [],
   "source": [
    "# display snow water equivalent from assimilated run, open loop run, and observed\n",
    "f,ax = plt.subplots(figsize=(10,7))\n",
    "ax.plot(assim_dates,np.sum(model_assim.we,axis=0),label = \"Assimilation\")\n",
    "ax.plot(assim_dates,np.sum(model_openloop.we,axis=0), label = \"Open Loop\")\n",
    "state_df[\"swe\"].plot(label = \"Observed\")\n",
    "ax.set_ylabel(\"SWE [mm]\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZnFyduXwPZJ"
   },
   "outputs": [],
   "source": [
    "# display top layer soil moisture from assimilated run, open loop run, and observed\n",
    "f,ax = plt.subplots(figsize=(10,7))\n",
    "ax.plot(assim_dates,model_assim.sm, label = \"Assimilation\")\n",
    "ax.plot(assim_dates,model_openloop.sm, label = \"Open Loop\")\n",
    "state_df[\"ssm\"].plot(label = \"Observed\")\n",
    "ax.set_ylabel(\"Soil moisture [mm]\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eiLJEPCw73p"
   },
   "outputs": [],
   "source": [
    "# display streamflow from assimilated run, open loop run, and observed\n",
    "f,ax = plt.subplots(figsize=(10,7))\n",
    "\n",
    "ax.plot(assim_dates, assim_q, label = \"Assimilation\")\n",
    "ax.plot(assim_dates, openloop_q, label = \"Open Loop\")\n",
    "obs_assim[\"discharge\"].plot(label = \"Observed\")\n",
    "ax.legend()\n",
    "ax.set_ylabel(\"Discharge [cms]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8i-gsusZx0p"
   },
   "outputs": [],
   "source": [
    "# create a dataframe of the simulated values\n",
    "sim_df = pd.DataFrame({\"assimilated\":assim_q, \"openloop\": openloop_q},index=assim_dates) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YaidQMbZ8aB"
   },
   "outputs": [],
   "source": [
    "# join the dataframes together to align dates\n",
    "joined = pd.concat([sim_df,obs_assim[\"discharge\"]], axis=1)\n",
    "joined = joined.loc[joined.index >= \"2016-01-01\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yr_ReAnuZpJG"
   },
   "outputs": [],
   "source": [
    "# evaluate the accuracy\n",
    "nse_assim = nse(joined[\"assimilated\"],joined[\"discharge\"])\n",
    "nse_openloop = nse(joined[\"openloop\"],joined[\"discharge\"])\n",
    "\n",
    "print(f\"Assimilated NSE: {nse_assim:.4f}\")\n",
    "print(f\"Open Loop NSE: {nse_openloop:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4_j_dG5WJig"
   },
   "source": [
    "As we can see, the data assimilation process improved our results (although not as much as we would like...). In reality the network is probably very regulated and any amount of calibration/assimilation cannot approve results and information on interventions can help improve results. Nevertheless, this illustrates the process of data assimilation for hydrology modeling."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP2rqLG7NTFbjpfcgK2KGBj",
   "collapsed_sections": [],
   "name": "Lab 15 - Simple data assimilation with remote sensing.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
